{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAN_FINANCIALipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvuSEluRnoV7SSDUXYFAIC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archanaqre/Deployment-flask/blob/master/HAN_FINANCIALipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E2qAYjYsMvL",
        "outputId": "4538fca6-c0c7-4644-a232-9fa62e9ee7a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive.zip\n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# unzip glove.6B.100d.txt\n",
        "!unzip /content/archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import string\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "SVZlcUi-v7w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import urllib\n",
        "import gzip\n",
        "from IPython.display import display, HTML"
      ],
      "metadata": {
        "id": "V6bUu-Rfv9zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "OL_OSGvYwAmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pip\n",
        "import theano\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
        "from keras import initializers as initializers, regularizers, constraints\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Embedding, Dropout, LSTM, GRU, Bidirectional, TimeDistributed\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "metadata": {
        "id": "AorpfQTvwDaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanString(review, stopWords):  #----- REVIEW ? -----#\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  returnString = \"\"\n",
        "  sentence_token = tokenize.sent_tokenize(review)\n",
        "  idx_list = []\n",
        "  for j in range(len(sentence_token)):\n",
        "    single_sentence = tokenize.word_tokenize(sentence_token[j])\n",
        "    sentences_filtered = [(idx, lemmatizer.lemmatize(w.lower())) for idx, w in enumerate(single_sentence) if w.lower() not in stopWords and w.isalnum()]\n",
        "    idx_list.append([x[0] for x in sentences_filtered])\n",
        "    word_list = [x[1] for x in sentences_filtered]\n",
        "    returnString = returnString + ' '.join(word_list) + ' . '\n",
        "\n",
        "  return returnString, idx_list\n",
        "\n",
        "\n",
        "def split_df(dataframe, column_name, training_split = 0.6, validation_split = 0.2, test_split = 0.2):\n",
        "  if training_split + validation_split + test_split != 1.0:\n",
        "    raise ValueError(\"Split parameter sum should be equal to 1.0\")\n",
        "\n",
        "  total = len(dataframe.index)\n",
        "\n",
        "  train = dataframe.reset_index().groupby(column_name).apply(lambda x: x.sample(frac=training_split))\\\n",
        "    .reset_index(drop=True).set_index('index')\n",
        "  train = train.sample(frac=1)\n",
        "  temp_df = dataframe.drop(train.index)\n",
        "  validation = temp_df.reset_index().groupby(column_name)\\\n",
        "    .apply(lambda x: x.sample(frac=validation_split/(test_split+validation_split)))\\\n",
        "           .reset_index(drop=True).set_index('index')\n",
        "  validation = validation.sample(frac=1)\n",
        "  test = temp_df.drop(validation.index)\n",
        "  test = test.sample(frac=1)\n",
        "    \n",
        "  print('Total: ', len(dataframe))\n",
        "  print('Training: ', len(train), ', Percentage: ', len(train)/len(dataframe))\n",
        "  print('Validation: ', len(validation), ', Percentage: ', len(validation)/len(dataframe))\n",
        "  print('Test:', len(test), ', Percentage: ', len(test)/len(dataframe))\n",
        "\n",
        "  return train, validation, test\n",
        "\n",
        "def wordToSeq(text,word_index,max_sentences,max_words,max_features):\n",
        "    sentences = tokenize.sent_tokenize(text)\n",
        "    data = np.zeros((max_sentences, max_words), dtype='int32')\n",
        "    for j, sent in enumerate(sentences):\n",
        "        if j< max_sentences:\n",
        "            wordTokens = tokenize.word_tokenize(sent.rstrip('.'))\n",
        "            wordTokens = [w for w in wordTokens]\n",
        "            k=0\n",
        "            for _, word in enumerate(wordTokens):\n",
        "                try:\n",
        "                    if k<max_words and word_index[word]<max_features:\n",
        "                        data[j,k] = word_index[word]\n",
        "                        k=k+1\n",
        "                except:\n",
        "                    pass\n",
        "    return data\n",
        "\n",
        "def to_categorical(series,class_dict):\n",
        "    n_classes = len(class_dict)\n",
        "    new_dict = {}\n",
        "    for key,value in class_dict.items():\n",
        "        cat_list = [0] * n_classes\n",
        "        cat_list[key] = 1\n",
        "        new_dict[key] = cat_list\n",
        "    y_cat = []\n",
        "    for key,value in series.iteritems():\n",
        "        y_cat.append(new_dict[value])\n",
        "    return np.array(y_cat)"
      ],
      "metadata": {
        "id": "A4KLVy8fw00H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    Hierarchial Attention Layer as described by Hierarchical Attention Networks for Document Classification(2016)\n",
        "    - Yang et. al.\n",
        "    Source: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
        "    Theano backend\n",
        "    \"\"\"\n",
        "    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n",
        "        # Initializer \n",
        "        self.supports_masking = True\n",
        "        self.return_coefficients = return_coefficients\n",
        "        self.init = initializers.get('glorot_uniform') # initializes values with uniform distribution\n",
        "        self.attention_dim = attention_dim\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Builds all weights\n",
        "        # W = Weight matrix, b = bias vector, u = context vector\n",
        "        assert len(input_shape) == 3\n",
        "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n",
        "        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n",
        "        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n",
        "        self._trainable_weights = [self.W, self.b, self.u]\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, hit, mask=None):\n",
        "        # Here, the actual calculation is done\n",
        "        uit = K.bias_add(K.dot(hit, self.W),self.b)\n",
        "        uit = K.tanh(uit)\n",
        "        \n",
        "        ait = K.dot(uit, self.u)\n",
        "        ait = K.squeeze(ait, -1)\n",
        "        ait = K.exp(ait)\n",
        "        \n",
        "        if mask is not None:\n",
        "            ait *= K.cast(mask, K.floatx())\n",
        "\n",
        "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        ait = K.expand_dims(ait)\n",
        "        weighted_input = hit * ait\n",
        "        \n",
        "        if self.return_coefficients:\n",
        "            return [K.sum(weighted_input, axis=1), ait]\n",
        "        else:\n",
        "            return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_coefficients:\n",
        "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
        "        else:\n",
        "            return input_shape[0], input_shape[-1]\n",
        "\n",
        "    #----- CODE FOR OVERRIDE -----#\n",
        "    \n",
        "    def get_config(self):\n",
        "      config = super().get_config()\n",
        "      config.update({\n",
        "          'attention_dim' : self.attention_dim,\n",
        "          'return_coefficients' : self.return_coefficients\n",
        "      })\n",
        "      return config"
      ],
      "metadata": {
        "id": "GgUpUKAaw7k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df=pd.read_csv('Fin.csv')\n",
        "display(data_df)\n",
        "data_df['Text']=data_df['Text'].apply(str)\n",
        "result=data_df.dtypes\n",
        "print(result)"
      ],
      "metadata": {
        "id": "d2qt9rIextrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Compute average number of words in each sentence and average number of sentences in each document.\n",
        "\"\"\"\n",
        "n_sent = 0\n",
        "n_words = 0\n",
        "for i in range(data_df.shape[0]):\n",
        "    sent = tokenize.sent_tokenize(data_df.loc[i,'Text'])\n",
        "    for satz in sent:\n",
        "        n_words += len(tokenize.word_tokenize(satz))\n",
        "    n_sent += len(sent)\n",
        "    \n",
        "print(\"Average number of words in each sentence: \",round(n_words/n_sent))\n",
        "print(\"Average number of sentences in each document: \", round(n_sent/data_df.shape[0]))"
      ],
      "metadata": {
        "id": "775XGbgI2cIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_FEATURES = 200000 # maximum number of unique words that should be included in the tokenized word index\n",
        "MAX_SENTENCE_NUM = 40 # maximum number of sentences in one document\n",
        "MAX_WORD_NUM = 50     # maximum number of words in each sentence\n",
        "EMBED_SIZE = 100      # vector size of word embedding"
      ],
      "metadata": {
        "id": "04nqRvgG5OB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cleans raw data using the cleanString() function from above.\n",
        "English stopwords are used from nltk library.\n",
        "Cleaned dataset is saved in 'data_cleaned' pandas dataframe.\n",
        "Labels are converted to numbers,\n",
        "\"\"\"\n",
        "articles = []\n",
        "n = data_df['Text'].shape[0]\n",
        "col_number = data_df.columns.get_loc('Text')\n",
        "stopWords = set(stopwords.words('english'))\n",
        "data_cleaned = data_df.copy()\n",
        "for i in range(n):\n",
        "    temp_string,idx_string = cleanString(data_df.iloc[i,col_number],stopWords)\n",
        "    articles.append(temp_string)\n",
        "    print(str(i+1)+' of '+str(n)+\" articles cleaned.\",end='\\r')\n",
        "    \n",
        "data_cleaned.loc[:,'Text'] = pd.Series(articles,index=data_df.index)\n",
        "data_cleaned.loc[:,'Category'] = pd.Categorical(data_cleaned.Category)\n",
        "data_cleaned['Code'] = data_cleaned.Category.cat.codes\n",
        "categoryToCode = dict( enumerate(data_cleaned['Category'].cat.categories))\n",
        "\n",
        "data_cleaned.head()"
      ],
      "metadata": {
        "id": "OGmptqvz5SZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Using the keras Tokenizer class a word index is built.\n",
        "The most 'MAX_FEATURES' used words are tokenized to a number.\n",
        "this dictionary is saved in word_index\n",
        "\"\"\"\n",
        "texts = []\n",
        "n = data_cleaned['Text'].shape[0]\n",
        "for i in range(n):\n",
        "    s = data_cleaned['Text'].iloc[i]\n",
        "    s = ' '.join([word.strip(string.punctuation) for word in s.split() if word.strip(string.punctuation) is not \"\"])\n",
        "    texts.append(s)\n",
        "tokenizer = Tokenizer(num_words=MAX_FEATURES,lower=True, oov_token=None)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "av4mF7e85hA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "A pre-trained word to vector is used from GloVe by Pennington et. al.\n",
        "Source: https://nlp.stanford.edu/projects/glove/\n",
        "The data was trained on wikipedia articles. Each word is described by a 100d vector.\n",
        "\"\"\"\n",
        "\n",
        "# Load word vectors from pre-trained dataset\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(os.getcwd(), 'glove.6B.100d.txt'),encoding='UTF-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# Search words in our word index in the pre-trained dataset\n",
        "# Create an embedding matrix for our bbc dataset\n",
        "min_wordCount = 0\n",
        "absent_words = 0\n",
        "small_words = 0\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
        "word_counts = tokenizer.word_counts\n",
        "for word, i in word_index.items():\n",
        "    if word_counts[word] > min_wordCount:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        else:\n",
        "            absent_words += 1\n",
        "    else:\n",
        "        small_words += 1\n",
        "print('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)),\n",
        "      '% of total words')\n",
        "print('Words with '+str(min_wordCount)+' or less mentions', small_words, 'which is', \"%0.2f\" % (small_words * 100 / len(word_index)),\n",
        "      '% of total words')"
      ],
      "metadata": {
        "id": "UVW_5nL15oHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Split Pandas Dataframe into train, validation and testset.\n",
        "Convert data to keras conforming form\n",
        "\"\"\"\n",
        "print(categoryToCode)\n",
        "train, validation, test = split_df(data_cleaned, 'Code',0.8,0.1,0.1)\n",
        "\n",
        "#Training\n",
        "paras = []\n",
        "for i in range(train['Text'].shape[0]):\n",
        "    sequence = wordToSeq(train['Text'].iloc[i],word_index,MAX_SENTENCE_NUM,MAX_WORD_NUM,MAX_FEATURES)\n",
        "    paras.append(sequence)\n",
        "x_train = np.array(paras)\n",
        "y_train = to_categorical(train['Code'],categoryToCode)\n",
        "\n",
        "#Validation\n",
        "paras = []\n",
        "for i in range(validation['Text'].shape[0]):\n",
        "    sequence = wordToSeq(validation['Text'].iloc[i],word_index,MAX_SENTENCE_NUM,MAX_WORD_NUM,MAX_FEATURES)\n",
        "    paras.append(sequence)\n",
        "x_val = np.array(paras)\n",
        "y_val = to_categorical(validation['Code'],categoryToCode)\n",
        "\n",
        "#Test\n",
        "paras = []\n",
        "for i in range(test['Text'].shape[0]):\n",
        "    sequence = wordToSeq(test['Text'].iloc[i],word_index,MAX_SENTENCE_NUM,MAX_WORD_NUM,MAX_FEATURES)\n",
        "    paras.append(sequence)\n",
        "x_test = np.array(paras)\n",
        "y_test = to_categorical(test['Code'],categoryToCode)"
      ],
      "metadata": {
        "id": "mOsfHU6f5xMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create Keras functional model for hierarchical attention network\n",
        "\"\"\"\n",
        "embedding_layer = Embedding(len(word_index) + 1,EMBED_SIZE,weights=[embedding_matrix], \n",
        "                            input_length=MAX_WORD_NUM, trainable=False,name='word_embedding')\n",
        "\n",
        "# Words level attention model\n",
        "word_input = Input(shape=(MAX_WORD_NUM,), dtype='int32',name='word_input')\n",
        "word_sequences = embedding_layer(word_input)\n",
        "word_gru = Bidirectional(GRU(50, return_sequences=True),name='word_gru')(word_sequences)\n",
        "word_dense = Dense(100, activation='relu', name='word_dense')(word_gru) \n",
        "word_att,word_coeffs = AttentionLayer(EMBED_SIZE,True,name='word_attention')(word_dense)\n",
        "wordEncoder = Model(inputs = word_input,outputs = word_att)\n",
        "\n",
        "# Sentence level attention model\n",
        "sent_input = Input(shape=(MAX_SENTENCE_NUM,MAX_WORD_NUM), dtype='int32',name='sent_input')\n",
        "sent_encoder = TimeDistributed(wordEncoder,name='sent_linking')(sent_input)\n",
        "sent_gru = Bidirectional(GRU(50, return_sequences=True),name='sent_gru')(sent_encoder)\n",
        "sent_dense = Dense(100, activation='relu', name='sent_dense')(sent_gru) \n",
        "sent_att,sent_coeffs = AttentionLayer(EMBED_SIZE,return_coefficients=True,name='sent_attention')(sent_dense)\n",
        "sent_drop = Dropout(0.5,name='sent_dropout')(sent_att)\n",
        "preds = Dense(2, activation='sigmoid',name='output')(sent_drop)\n",
        "\n",
        "# Model compile\n",
        "model = Model(sent_input, preds)\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "print(wordEncoder.summary())\n",
        "print(model.summary())\n",
        "\n",
        "plot_model(model, to_file='model.png',show_shapes=True)\n",
        "plot_model(wordEncoder, to_file='wordEncoder.png',show_shapes=True)"
      ],
      "metadata": {
        "id": "WaEUL9mA59AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#epochs_1 = 7\n",
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=6, batch_size=50)\n",
        "\n",
        "print(history.history.keys())\n",
        "# Plot of accuracy in each epoch\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XWHZCEwR6CTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(x_test,y_test)\n",
        "print(\"Test set accuracy: \",acc)\n",
        "print(\"Test set loss: \", loss)"
      ],
      "metadata": {
        "id": "mzJe9E6qE7Bc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}